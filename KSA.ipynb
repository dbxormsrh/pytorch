{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training fasttext model\n",
    "\n",
    "Using corpus : https://81675795.ucloudcdnglobal.com/122/NIKL_MP_v1.1.pdf\n",
    "https://kli.korean.go.kr/corpus/main/requestMain.do -> 형태 분석 말뭉치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hgtk\n",
    "def decompose(form):\n",
    "    word = ''\n",
    "    try:\n",
    "        for s in form:\n",
    "            if s == ' ':\n",
    "                word += ''\n",
    "            elif hgtk.checker.is_hangul(s):\n",
    "                a, b, c = hgtk.letter.decompose(s)\n",
    "                if not a:\n",
    "                    a = '-'\n",
    "                if not b:\n",
    "                    b = '-'\n",
    "                if not c:\n",
    "                    c = '-'\n",
    "                word = word + a + b + c\n",
    "    except e:\n",
    "        print(e)\n",
    "        print(f'except: {form}')\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name = '../NIKL_MP_CSV/NXMP1902008040_{}.csv'    #문장이 sentence로, 문장을 형태소로 분석한 내용이 form으로 들어가 있음\n",
    "with open('decomposed_sent.txt', 'w') as f:\n",
    "    for i in range(5):\n",
    "        df = pd.read_csv(df_name.format(i + 1), skipinitialspace=True, usecols=('sentence_id', 'form'))\n",
    "        sent_form={}\n",
    "        pre_sent = ''\n",
    "        \n",
    "        for value in tqdm(df.values):\n",
    "            sent, form = value[0], value[1]\n",
    "            sent = re.sub('ㄱ-ㅎ')\n",
    "            if pre_sent != sent:\n",
    "                sent_form[sent] = []\n",
    "                pre_sent = sent\n",
    "            sent_form[sent].append(form)\n",
    "            \n",
    "        for form in tqdm(sent_form.values()):\n",
    "            f.write(' '.join(decompose(form)) + '\\n')      #decompose('안녕하세요') == 'ㅇㅏㄴㄴㅕㅇㅎㅏㅅㅔ-ㅇㅛ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model = fasttext.train_unsupervised('decomposed_sent.txt', dim = 100, epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model.save_model('fasttext_with_NIKL_MP_CSV.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing training data with Khaiii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from khaiii import KhaiiiApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "khaiii_api = KhaiiiApi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt', filename = 'ratings.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table('./ratings.txt')\n",
    "df = df.dropna(how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['document'] = df['document'].str.replace('[^ㄱ-ㅎ가-힣]', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(index=df[df['document'].str.rstrip() == ''].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('morphs_label.txt', 'w') as out:\n",
    "    for idx, row in tqdm(df.iterrows()):\n",
    "        sent = row['document'].rstrip()\n",
    "        sent = ' '.join(sent.split())\n",
    "        label = row['label']\n",
    "        morphs = ''\n",
    "        try:\n",
    "            for word in khaiii_api.analyze(sent):\n",
    "                for m in word.morphs:\n",
    "                    morphs += m.lex + ' '\n",
    "        except:\n",
    "            print(idx, row)\n",
    "            break\n",
    "        out.write(morphs + '\\t' + str(label) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('./morphs_label.txt', 'r')\n",
    "ori_data = []\n",
    "for line in file:\n",
    "    morphs, label = line.split('\\t')\n",
    "    ori_data.append((morphs, int(label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0\n",
    "f = 0\n",
    "for (_, y) in ori_data:\n",
    "    if y:\n",
    "        t += 1\n",
    "    else:\n",
    "        f += 1\n",
    "\n",
    "print(f'label \\'1\\' : {t} / label \\'0\\' : {f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_len = [len(s[0].split()) for s in ori_data]\n",
    "pd.Series(sent_len).hist()\n",
    "plt.show()\n",
    "pd.Series(sent_len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {}\n",
    "\n",
    "for n in sent_len:\n",
    "    if n in dic:\n",
    "        dic[n] += 1\n",
    "    else:\n",
    "        dic[n] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cnt = 0\n",
    "keys = sorted(dic.keys())\n",
    "f = True\n",
    "for key in keys:\n",
    "    cnt = dic[key]\n",
    "    total_cnt += cnt\n",
    "    if key >= 64:\n",
    "        print(total_cnt / len(ori_data) * 100)\n",
    "        break\n",
    "        \n",
    "    if (total_cnt / len(ori_data)) * 100 > 90 and f:\n",
    "        print(key, total_cnt)\n",
    "        f = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_model = fasttext.load_model(\"fasttext_with_NIKL_MP_CSV.bin\") # 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "from torch import nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir, num_word, transform = None, target_transform=None):\n",
    "        file = open(data_dir, 'r')\n",
    "        self.ori_data = []\n",
    "        for line in file:\n",
    "            morphs, label = line.split('\\t')\n",
    "            self.ori_data.append((morphs, int(label)))\n",
    "        self.data = self.ori_data\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.num_word = num_word\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        sent = self.data[i][0]\n",
    "        padded_vec = torch.zeros((self.num_word, fast_model.get_dimension()), dtype = torch.float32)\n",
    "        \n",
    "        sent2vec = []\n",
    "        x = ''\n",
    "        for m in sent.split():\n",
    "            if m.rstrip():\n",
    "                sent2vec.append(fast_model.get_word_vector(decompose(m)))\n",
    "        sent2vec = np.array(sent2vec)\n",
    "        len_sent = len(sent2vec)\n",
    "        if len_sent > self.num_word:\n",
    "            len_sent = self.num_word\n",
    "        padded_vec[(self.num_word - len_sent):] = torch.from_numpy(sent2vec[:len_sent])\n",
    "         \n",
    "            \n",
    "        label = torch.tensor(self.data[i][1], dtype = torch.float32)\n",
    "        return (padded_vec, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset('./morphs_label.txt', num_word = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(dataset) * 0.8)\n",
    "valid_size = len(dataset) - train_size\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = random_split(dataset, [train_size, valid_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size = batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size = batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_sent, train_label = next(iter(train_dataloader))\n",
    "print(train_sent)\n",
    "print(train_label)\n",
    "print(train_sent.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_dim):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=self.input_size, hidden_size=self.hidden_size, num_layers=num_layers,batch_first = True)\n",
    "        \n",
    "        self.linear = nn.Linear(self.hidden_size, self.output_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        drop_out = self.dropout(lstm_out)\n",
    "        re_drop_out = drop_out.reshape([-1, self.hidden_size])\n",
    "            \n",
    "        linear_out = self.linear(re_drop_out)\n",
    "        \n",
    "        sig_out = self.sig(linear_out).reshape([batch_size, -1])[:, -1]\n",
    "        \n",
    "        return sig_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 2\n",
    "input_size = 100\n",
    "hidden_size = 128\n",
    "output_dim = 1\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = SentimentLSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, output_dim=output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "clip = 5\n",
    "epochs = 5\n",
    "\n",
    "loss_func = nn.BCELoss()#.to(device)\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(), lr = lr)\n",
    "\n",
    "def acc(pred, label):\n",
    "    correct = torch.eq(pred.round(), label).sum().item()\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_tr_acc, epoch_tr_loss = [], []\n",
    "epoch_vl_acc, epoch_vl_loss = [],[]\n",
    "for epoch in range(epochs):\n",
    "    train_losses = []\n",
    "    train_acc = 0.0\n",
    "    lstm_model.train()\n",
    "    #h = lstm_model.init_hidden(batch_size, device)\n",
    "\n",
    "    for inputs, labels in tqdm(train_dataloader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        #h = tuple([each.data for each in h])\n",
    "        \n",
    "        pred = lstm_model(inputs)\n",
    "        \n",
    "        loss = loss_func(pred, labels)\n",
    "        loss.backward()\n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        accuracy = acc(pred, labels)\n",
    "\n",
    "        train_acc += accuracy\n",
    "        \n",
    "        nn.utils.clip_grad_norm_(lstm_model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    epoch_train_loss = np.mean(train_losses)\n",
    "    epoch_train_acc = train_acc/len(train_dataloader.dataset)\n",
    "    epoch_tr_loss.append(epoch_train_loss)\n",
    "    epoch_tr_acc.append(epoch_train_acc)\n",
    "    \n",
    "    val_losses = []\n",
    "    val_acc = 0.0\n",
    "    lstm_model.eval()\n",
    "    #val_h = lstm_model.init_hidden(batch_size, device)\n",
    "\n",
    "    for inputs, labels in tqdm(valid_dataloader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        #val_h = tuple([each.data for each in val_h])\n",
    "        pred = lstm_model(inputs)\n",
    "\n",
    "        val_loss = loss_func(pred, labels.float())\n",
    "        val_losses.append(val_loss.item())\n",
    "        accuracy = acc(pred, labels)\n",
    "\n",
    "        val_acc += accuracy\n",
    "    \n",
    "    epoch_val_loss = np.mean(val_losses)\n",
    "    epoch_val_acc = val_acc/len(valid_dataloader.dataset)\n",
    "    epoch_vl_loss.append(epoch_val_loss)\n",
    "    epoch_vl_acc.append(epoch_val_acc)\n",
    "\n",
    "    print(f'Epoch {epoch+1}') \n",
    "    print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')\n",
    "    print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n",
    "    print(25*'==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lstm_model, f='./k_sentiment_Fasttext_LSTM.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_x = torch.load('./k_sentiment_Fasttext_LSTM.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentLSTM(\n",
       "  (lstm): LSTM(100, 128, num_layers=2, batch_first=True)\n",
       "  (linear): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epoch_tr_acc, label='Train Acc')\n",
    "plt.plot(epoch_vl_acc, label='Validation Acc')\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "    \n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epoch_tr_loss, label='Train loss')\n",
    "plt.plot(epoch_vl_loss, label='Validation loss')\n",
    "plt.title(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sent(sent, fast_model, lstm_model, khaiii, num_word):\n",
    "    morphs = []\n",
    "    try:\n",
    "        for word in khaiii.analyze(sent):\n",
    "            for m in word.morphs:\n",
    "                morphs.append(m.lex)\n",
    "    except:\n",
    "        print('Can\\'t analyze sentence')\n",
    "        return -1\n",
    "    \n",
    "    \n",
    "    if len(morphs) > num_word:\n",
    "        morphs = morphs[:num_word]\n",
    "        \n",
    "    sent_vec = np.zeros((num_word, fast_model.get_dimension()), dtype=np.float32)\n",
    "    \n",
    "    for i, m in zip(range(num_word), morphs):\n",
    "        word_vec = fast_model.get_word_vector(decompose(m)).astype(np.float32)\n",
    "        sent_vec[-(i + 1)] = word_vec\n",
    "    \n",
    "    sent_tensor = torch.from_numpy(sent_vec)\n",
    "    sent_tensor = sent_tensor.reshape([1, num_word, fast_model.get_dimension()])\n",
    "\n",
    "    device = (\n",
    "        \"cuda\"\n",
    "        if torch.cuda.is_available()\n",
    "        else \"mps\"\n",
    "        if torch.backends.mps.is_available()\n",
    "        else \"cpu\"\n",
    "    )\n",
    "    \n",
    "    if device == 'cuda':\n",
    "        sent_tensor = sent_tensor.to(device)\n",
    "    \n",
    "    lstm_model.eval()\n",
    "    \n",
    "    pred = lstm_model(sent_tensor)\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = analyze_sent('', fast_model, model_x, khaiii_api, 64)\n",
    "print(pred)\n",
    "      \n",
    "if round(pred.item()) > 0.5:\n",
    "    print('긍정')\n",
    "else :\n",
    "    print('부정')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
