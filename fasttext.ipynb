{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hgtk\n",
    "from tqdm import tqdm\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose(forms:list):\n",
    "    word = ''\n",
    "    for form in forms:\n",
    "        try:\n",
    "            if hgtk.checker.is_hangul(form):\n",
    "                for s in form:\n",
    "                    a, b, c = hgtk.letter.decompose(s)\n",
    "                    if not a:\n",
    "                        a = '-'\n",
    "                    if not b:\n",
    "                        b = '-'\n",
    "                    if not c:\n",
    "                        c = '-'\n",
    "                    word = word + a + b + c\n",
    "        except TypeError as e:\n",
    "            print(form)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose(jamo_sequence:list):\n",
    "    tokenized_jamo = []\n",
    "    index = 0\n",
    "\n",
    "    while index < len(jamo_sequence):\n",
    "        if not hgtk.checker.is_hangul(jamo_sequence[index]):\n",
    "            tokenized_jamo.append(jamo_sequence[index])\n",
    "            index = index + 1\n",
    "\n",
    "        else:\n",
    "            tokenized_jamo.append(jamo_sequence[index:index + 3])\n",
    "            index = index + 3\n",
    "\n",
    "    word = ''\n",
    "    try:\n",
    "        for jamo in tokenized_jamo:\n",
    "\n",
    "            if len(jamo) == 3:\n",
    "                if jamo[2] == \"-\":\n",
    "                    word = word + hgtk.letter.compose(jamo[0], jamo[1])\n",
    "                else:\n",
    "                    word = word + hgtk.letter.compose(jamo[0], jamo[1], jamo[2])\n",
    "            else:\n",
    "                word = word + jamo\n",
    "\n",
    "    except Exception as exception:\n",
    "        if type(exception).__name__ == 'NotHangulException':\n",
    "            return jamo_sequence\n",
    "\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_model = fasttext.load_model(\"fasttext_jn.bin\") # 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(word_sequence):\n",
    "    return [(compose(word), similarity) for (similarity, word) in word_sequence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.23002268,  0.39805633, -0.595214  , -0.4201808 ,  0.06841939,\n",
       "       -0.23568943,  0.33469296,  0.09796971,  0.10487619,  0.17936371,\n",
       "       -0.04858068,  0.11436544, -0.18841325, -0.1625356 ,  0.03736197,\n",
       "        0.23201014, -0.30488858, -0.11117312, -0.21404381, -0.01474717,\n",
       "        0.13891964, -0.6124201 , -0.3584761 , -0.41327453, -0.21037365,\n",
       "        0.30367267, -0.06418942,  0.20489423, -0.15140772,  0.34683898,\n",
       "       -0.08985352,  0.21257268, -0.07074974,  0.20376125, -0.09280302,\n",
       "       -0.33641914, -0.32314423, -0.19665879, -0.25710115,  0.04477051,\n",
       "       -0.0592188 , -0.04977519,  0.60128856, -0.15175302, -0.6904205 ,\n",
       "       -0.4166109 ,  0.2812419 ,  0.1937623 , -0.70656985, -0.4241435 ,\n",
       "        0.8790712 , -0.02911854, -0.02107281,  0.7127032 ,  0.61294305,\n",
       "       -0.10936469,  0.17948501, -0.3176769 ,  0.06604326,  0.24650899,\n",
       "       -0.4777825 ,  0.52821845, -0.32548603, -0.3195687 ,  0.32848126,\n",
       "        0.50072765,  0.13473417,  0.10986972, -0.09630211,  0.520136  ,\n",
       "        0.27817407,  0.08595612, -0.08750131,  0.21907571, -0.01260473,\n",
       "        0.01257326, -0.04710799, -0.09702913,  0.2330503 , -0.14492422,\n",
       "        0.41477886, -0.46510795, -0.16225938,  0.36350194,  0.17462806,\n",
       "       -0.3060407 ,  0.05309338, -0.1311184 , -0.21973711, -0.8013788 ,\n",
       "       -0.31104523, -0.31066498,  0.12453254, -0.00811013, -0.32685027,\n",
       "        0.3073702 ,  0.4541527 ,  0.27231634,  0.45880464,  0.37605458],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_model.get_word_vector(decompose(['제주도']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader, Dataset, TensorDataset\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_dir, num_word, transform = None, target_transform=None):\n",
    "        self.df = pd.read_csv(csv_dir).sample(frac=1)[:5120]\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.num_word = num_word\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        sent = self.df['morphologized_sent'].iloc[i]\n",
    "        label = self.df['label'].iloc[i]\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        padded_vec = torch.zeros((self.num_word, fast_model.get_dimension()), dtype = torch.float32)\n",
    "        \n",
    "        sent2vec = [] \n",
    "        for w in sent:\n",
    "            if w.rstrip():\n",
    "                sent2vec.append(fast_model.get_word_vector(decompose(w)))\n",
    "        sent2vec = np.array(sent2vec)\n",
    "        len_sent = len(sent2vec)\n",
    "        if len_sent > self.num_word:\n",
    "            len_sent = self.num_word\n",
    "        padded_vec[(self.num_word - len_sent):] = torch.from_numpy(sent2vec[:len_sent])\n",
    "        \n",
    "        return (padded_vec, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset('./morphologized_ratings.csv', num_word=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent_len = [len(s.split()) for s in dataset.df['morphologized_sent']]\n",
    "pd.Series(sent_len).hist()\n",
    "plt.show()\n",
    "pd.Series(sent_len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sent_len' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d309db952fd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sent_len' is not defined"
     ]
    }
   ],
   "source": [
    "sr = pd.Series(sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = defaultdict(int)\n",
    "for i in sr:\n",
    "    dic[i] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic\n",
    "i = 0\n",
    "t = 1\n",
    "for k, v in dic.items():\n",
    "    i += v\n",
    "    x = i / 5120 * 100\n",
    "    if k == 32:\n",
    "        print(x)\n",
    "    \n",
    "    if x//10 == t:\n",
    "        print(k, v, i, x)\n",
    "        t += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 4096\n",
    "valid_size = 1024\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_size = int(len(dataset) * 0.7)\n",
    "valid_size = len(dataset) - train_size\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = random_split(dataset, [train_size, valid_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size = batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size = batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 32, 100])\n"
     ]
    }
   ],
   "source": [
    "train_sent, train_label = next(iter(train_dataloader))\n",
    "print(train_sent.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_dim):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=self.input_size, hidden_size=self.hidden_size, num_layers=num_layers,batch_first = True)\n",
    "        \n",
    "        self.linear = nn.Linear(self.hidden_size, self.output_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        lstm_out, (hn, cn) = self.lstm(x, hidden)\n",
    "        \n",
    "        drop_out = self.dropout(lstm_out)\n",
    "        re_drop_out = drop_out.reshape([-1, self.hidden_size])\n",
    "        \n",
    "        linear_out = self.linear(re_drop_out)\n",
    "        \n",
    "        sig_out = self.sig(linear_out)\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1]\n",
    "        \n",
    "        return sig_out\n",
    "        \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        h0 = torch.zeros((self.num_layers, batch_size, self.hidden_size), dtype=torch.float32).to(device)\n",
    "        c0 = torch.zeros((self.num_layers, batch_size, self.hidden_size), dtype = torch.float32).to(device)\n",
    "        hidden = (h0, c0)\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 1\n",
    "input_size = 100\n",
    "hidden_size = 128\n",
    "output_dim = 1\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = SentimentLSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, output_dim=output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentLSTM(\n",
       "  (lstm): LSTM(100, 128, batch_first=True)\n",
       "  (linear): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "clip = 5\n",
    "epochs = 5\n",
    "\n",
    "loss_func = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(), lr = lr)\n",
    "\n",
    "def acc(pred, label):\n",
    "    pred = torch.round(pred.squeeze())\n",
    "    return torch.sum(pred == label.squeeze()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(dataloader, model):\n",
    "    \n",
    "    train_losses = []\n",
    "    train_acc = 0.0\n",
    "    model.train()\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    for inputs, labels in tqdm(dataloader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        lstm_model.zero_grad()\n",
    "\n",
    "        pred = model(inputs, hidden)\n",
    "        #print(pred.shape, labels.shape)\n",
    "        loss = loss_func(pred, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        accuracy = acc(pred, labels)\n",
    "\n",
    "        train_acc += accuracy\n",
    "\n",
    "\n",
    "    epoch_train_loss = np.mean(train_losses)\n",
    "    epoch_train_acc = train_acc/len(train_dataloader.dataset)\n",
    "        \n",
    "    return epoch_train_loss, epoch_train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_valid(dataloader, model):\n",
    "    val_losses = []\n",
    "    val_acc = 0.0\n",
    "    model.eval()\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    for inputs, labels in tqdm(dataloader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        pred = model(inputs, hidden)\n",
    "\n",
    "        val_loss = loss_func(pred.squeeze(), labels.float())\n",
    "        val_losses.append(val_loss.item())\n",
    "        accuracy = acc(pred, labels)\n",
    "\n",
    "        val_acc += accuracy\n",
    "\n",
    "    epoch_val_loss = np.mean(val_losses)\n",
    "    epoch_val_acc = val_acc/len(valid_dataloader.dataset)\n",
    "        \n",
    "    return epoch_val_acc, epoch_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [00:18<00:00, 13.71it/s]\n",
      "100%|██████████| 64/64 [00:03<00:00, 17.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "train_loss : 0.6929674204438925 val_loss : 0.5185546875\n",
      "train_accuracy : 50.8056640625 val_accuracy : 69.23658372834325\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [00:16<00:00, 15.88it/s]\n",
      "100%|██████████| 64/64 [00:03<00:00, 18.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n",
      "train_loss : 0.6931896188762039 val_loss : 0.5185546875\n",
      "train_accuracy : 50.5859375 val_accuracy : 69.23658307641745\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 145/256 [00:09<00:07, 15.80it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-63356035d4a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mepoch_vl_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_vl_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mepoch_train_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_train_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mepoch_tr_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_train_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mepoch_tr_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_train_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-497746d45cb9>\u001b[0m in \u001b[0;36mmodel_train\u001b[0;34m(dataloader, model)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "epoch_tr_acc, epoch_tr_loss = [], []\n",
    "epoch_vl_acc, epoch_vl_loss = [],[]\n",
    "for epoch in range(epochs):\n",
    "    epoch_train_loss, epoch_train_acc = model_train(train_dataloader, lstm_model)\n",
    "    epoch_tr_loss.append(epoch_train_loss)\n",
    "    epoch_tr_acc.append(epoch_train_acc)\n",
    "    \n",
    "    epoch_val_loss, epoch_val_acc = model_valid(valid_dataloader, lstm_model)\n",
    "    epoch_vl_loss.append(epoch_val_loss)\n",
    "    epoch_vl_acc.append(epoch_val_acc)\n",
    "    \n",
    "    \n",
    "    print(f'Epoch {epoch+1}') \n",
    "    print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')\n",
    "    print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n",
    "    print(25*'==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epoch_tr_acc, label='Train Acc')\n",
    "#plt.plot(epoch_vl_acc, label='Validation Acc')\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "    \n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epoch_tr_loss, label='Train loss')\n",
    "#plt.plot(epoch_vl_loss, label='Validation loss')\n",
    "plt.title(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lstm_model, './lstm_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
